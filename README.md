Introduction
3D Referring Expression Segmentation (3D-RES) is an emerging yet challenging task at the interaction of vision and language, which aims to precisely segment a target instance within a 3D point cloud based on a given natural language referring expression. However, most previous methods overlook multi-source ambiguities that are prevalent in real-world scenarios, including prompt, spatial, and annotation ambiguities. Prompt ambiguity arises from confusion between referent and target objects due to ambiguous language, spatial ambiguity results from viewpoint variations causing incomplete segmentation, annotation ambiguity stems from inconsistent or noisy labeling in training data. In this paper, we propose a novel 3D Ambiguity-Tolerant Referring Expression Segmentation (3D-ATRES), which explicitly models and mitigates multi-source ambiguities in 3D-RES. Specifically, we employ $TR^2$ Semantic Structurizer to transform free-form natural language into structured Target–Relation–Referent triples, thereby eliminating referential ambiguity. For spatial ambiguity, we introduce a Normal‑Aware Spatial Alignment that leverages surface normal cues to achieve viewpoint-consistent geometry alignment. To combat annotation ambiguity, we incorporate a Annotation Ambiguity Penalty, enabling the network to learn from noisy or inconsistent annotations in a probabilistic manner. Experiments on ScanRefer and Multi3DRefer show that 3D-ATRES achieves state-of-the-art performance, confirming the effectiveness of modeling ambiguity in 3D-RES. Code will be released upon acceptance.

<div align="center">
<h1>3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer</h1>

<a href="https://arxiv.org/abs/2501.01163"><img src="https://img.shields.io/badge/arXiv-2501.01163-b31b1b" alt="arXiv"></a>
<a href='https://huggingface.co/datasets/djiajunustc/3D-LLaVA-Data'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Processed_Data-blue'></a>
<a href="https://huggingface.co/djiajunustc/3D-LLaVA-7B-LoRA" target="_blank"><img src="https://img.shields.io/badge/Checkpoint-Orange" alt="checkpoint"></a>

[Jiajun Deng](https://djiajunustc.github.io/), [Tianyu He](https://www.microsoft.com/en-us/research/people/tianyuhe/), [Li Jiang](https://llijiang.github.io/), [Tianyu Wang](https://openreview.net/profile?id=~Tianyu_Wang5), [Feras Dayoub](https://ferasdayoub.com/), [Ian Reid](https://researchers.adelaide.edu.au/profile/ian.reid)
</div>

```bibtex
@inproceedings{deng20253dllava,
  title={3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer},
  author={Deng, Jiajun and He, Tianyu and Jiang, Li and Wang, Tianyu and Dayoub, Feras and Reid, Ian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```

## Overview
<div style="text-align: center;">
    <img src="docs/framework.png" alt="Dialogue_Teaser" width=100% >
</div>
3D-LLaVA (CVPR 2025) is 3D Large Multimodal Model that takes point clouds and text instruction as input to perform VQA, Dense Captioning and 3D Referring Segmentation. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description.


## Environment
We provide the Docker Image to run our 3D-LLaVA. Please run the following code to pull the docker image:
```
docker pull djiajun1206/3d-llava-slim
```

## Data
We conduct experiments with the scans data from Scannet, as well as the text description from ScanRefer, ScanQA, SQA3D, ReferIt3D and Multi3DRefer. To enable conventiently getting access to the data, we provide the [processed data](https://huggingface.co/datasets/djiajunustc/3D-LLaVA-Data). The data are supposed to be placed in ./playground, and the data structure is as follows:
```
3D-LLaVA # project root
|── playground
|   |── data
│   |   ├── scannet
│   |   │   ├── super_points
|   │   │   ├── train
|   │   │   ├── val
|   │   │   └── scannet_axis_align_matrix_trainval.pkl
│   |   ├── train_info
│   │   |   ├── scanqa_train_3d_llava.json
│   │   |   ├── sqa3d_train_3d_llava.json
│   │   |   ├── scan2cap_train_3d_llava.json
│   │   |   ├── ...
│   │   └── eval_info
│   │   |   ├── scanqa
│   │   |   ├── sqa3d
│   │   |   ├── densecap_scanrefer
│   │   |   ├── ...
```

## Training
We exploit LoRA tuning by default. Please train the 3D-LLaVA with:
```
./scripts/train/finetune-3d-llava-lora.sh
```

## Evaluation
We provide the scripts to evaluate our model on ScanQA, SQA3D, Scan2Cap, ScanRefer, Multi3DRefer. Please run:
```
./scripts/eval/multigpu_eval_sqa3d.sh

./scripts/eval/multigpu_eval_scanqa.sh

./scripts/eval/multigpu_eval_scan2cap.sh

./scripts/eval/multigpu_eval_scanrefer.sh

./scripts/eval/multigpu_eval_multi3drefer.sh
```

## Acknowledgements
Thanks to the following great repositories: [LLaVA](https://github.com/haotian-liu/LLaVA), [PonderV2](https://github.com/OpenGVLab/PonderV2), [OneFormer3d](https://github.com/filaPro/oneformer3d).
